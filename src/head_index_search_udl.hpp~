#include <cascade/object.hpp>
#include <chrono>
#include <immintrin.h> // needed to include this to make sure that the code compiles since in DiskANN/include/utils.h it uses this library.
#include "in_mem_data_store.h"
#include "in_mem_graph_store.h"
#include "index.h"
#include "serialize_utils.hpp"
#include <cascade/cascade_interface.hpp>
#include <cascade/service_types.hpp>
#include <cascade/user_defined_logic_interface.hpp>
#include <cascade/utils.hpp>
#include <iostream>
#include <map>
#include <memory>
#include <mutex>
#include <stdexcept>
#include <unordered_map>

namespace derecho {
namespace cascade {

#define MY_UUID "69eb06e2-017c-481b-8534-2e5dac301949"
#define MY_DESC                                                                \
  "UDL for searching the head index to find good starting points in clusters " \
  "for greedy search udl"
#define NEXT_UDL_SUBGROUP_ID 1
#define DATA_TYPE "float"
#define HEAD_INDEX_BEAM_SIZE 5
// beamsize is adjusttable, since we are just considering the nearest neighbor
// (k = 1) of head index search, it shouldn't matter too much past a certain
// point. Can definitely run experiements to verify tho.
  
std::string get_uuid();

std::string get_description();


template<typename data_type>
class HeadIndexSearchOCDPO : public DefaultOffCriticalDataPathObserver {
  class HeadIndexSearchThread {
  private:
    uint64_t my_thread_id;
    HeadIndexSearchOCDPO *parent;
    std::thread real_thread;
    bool running = false;

    std::condition_variable_any query_queue_cv;
    std::mutex query_queue_mutex;
    std::queue<std::shared_ptr<EmbeddingQuery<data_type>>> query_queue;

    void main_loop() {
      std::unique_lock lock(query_queue_mutex, std::defer_lock);
      while (running) {
        lock.lock();
        query_queue_cv.wait(lock, [&] { return !query_queue.empty(); });
        if (!running)
          break;
        auto query = query_queue.front();
        query_queue.pop();
        lock.unlock();
        // do search here
        auto [cluster_id, node_id] = head_index_search(query);
        parent->batch_thread->push(cluster_id, node_id, query);
      }

    }

    // searches the head index and returns a the (cluster (where the node
    // belongs + node id), with this info, you can trigger computation on next
    // udl. K = 1 for search
    std::pair<std::byte, uint32_t>
    head_index_search(std::shared_ptr<EmbeddingQuery<data_type>> &query) {
      std::vector<uint32_t> search_id_results(HEAD_INDEX_BEAM_SIZE);
      std::vector<float> search_dist_results(HEAD_INDEX_BEAM_SIZE);
      auto [hops, dist_cmps] = parent->head_index->search(query->get_embedding_ptr(), 1,
                                 HEAD_INDEX_BEAM_SIZE, search_id_results.data(),
                                 search_dist_results.data());
      uint32_t nearest_neighbor_index = search_id_results[0];
      return std::make_pair(parent->cluster_assignment[nearest_neighbor_index], parent->id_mapping[nearest_neighbor_index]);
    }

  public:
    HeadIndexSearchThread(uint64_t thread_id, HeadIndexSearchOCDPO *parent)
    : my_thread_id(thread_id), parent(parent) {}
    void start() {
      running = true;
      real_thread = std::thread(&HeadIndexSearchThread::main_loop, this);
    }
    void join() {
      if (real_thread.joinable()) real_thread.join();
    }
    void signal_stop() {
      std::scoped_lock l(query_queue_mutex);
      running = false;
      query_queue.push(nullptr);
      query_queue_cv.notify_all();
    }

    void push(std::shared_ptr<EmbeddingQuery<data_type>> query) {
      std::scoped_lock l(query_queue_mutex);
      query_queue.push(query);
      query_queue_cv.notify_all();      
    }      
  };
  uint64_t num_search_threads = 1;
  uint64_t next_search_thread = 0;
  std::vector<std::unique_ptr<HeadIndexSearchThread>> search_threads;

  /**
     this thread batches the result from the search thread based on the
     cluster_id and sends them to the next udl.
   */
  class BatchingThread {
  private:
    uint64_t my_thread_id;
    HeadIndexSearchOCDPO *parent;
    std::thread real_thread;
    bool running = false;

    std::unordered_map<
		       std::byte, std::unique_ptr<std::vector<
                        std::pair<uint32_t, std::shared_ptr<EmbeddingQuery<data_type>>>>>>
        cluster_queue; // for each cluster, there is a list of starting points
                       // and query to batch.
    std::condition_variable_any cluster_queue_cv;
    std::mutex cluster_queue_mutex;

    // for now, the main loop will just put the search results into emit_key_prefix_test = /anns/results
    void main_loop(DefaultCascadeContextType *typed_ctxt) {
      // TODO
      std::unique_lock<std::mutex> lock(cluster_queue_mutex, std::defer_lock);
      std::unordered_map<uint64_t, std::chrono::steady_clock::time_point>
      wait_time;
      auto batch_time = std::chrono::microseconds(parent->batch_time_us);
      while (running) {
        lock.lock();
        bool empty = true;
        for(auto& item : cluster_queue){
          if(!(item.second->empty())){
            empty = false;
            break;
          }
        }

        if(empty){
            cluster_queue_cv.wait_for(lock,batch_time);
        }
        
        if (!running) break;


      }
    }

  public:
    BatchingThread(uint64_t thread_id, HeadIndexSearchOCDPO *parent) : my_thread_id(thread_id), parent(parent), running(false) {}
    void start(DefaultCascadeContextType *typed_ctxt) {
      running = true;
      this->real_thread = std::thread(&BatchingThread::main_loop, this, typed_ctxt);
    }
    void join() {
      if (real_thread.joinable()) {
        real_thread.join();
      }
    }
    void signal_stop() {
      std::scoped_lock<std::mutex> l(cluster_queue_mutex);
      running = false;
      cluster_queue_cv.notify_all();
    }
    void push(std::byte cluster_id, uint32_t starting_point,
              std::shared_ptr<EmbeddingQuery<data_type>> query) {
      if (cluster_queue.count(cluster_id) == 0) {
        cluster_queue[cluster_id] = std::make_unique<std::vector<
								 std::pair<uint32_t, std::shared_ptr<EmbeddingQuery<data_type>>>>>();
        cluster_queue[cluster_id]->reserve(parent->max_batch_size);
      }
      cluster_queue[cluster_id]->push_back(
					   std::make_pair(starting_point, query));
    }
  };
  uint32_t num_pts;
  uint32_t dim;

  std::unique_ptr<diskann::Index<data_type>> head_index;
  bool cached_head_index = false; // if head index is loaded into mem or not

  // id_mapping is the mapping from the vector id of the head index to the
  // ids of the graph
  std::vector<uint32_t> id_mapping;
  // each byte represents the cluster assignment of the corresponding graph
  // vector id from the id_mapping. 
  std::vector<std::byte> cluster_assignment;

  // data here: num_pts (uint32_t), num_dim (uint32_t), data....
  std::string data_store_key = "/anns/head_index/data_store";
  std::string graph_store_key =
      "/anns/head_index/graph_store"; // how to store this we have to figure out
                                      // mate. maybe: num_pts, max_deg, [data]
                                      // where for each vector

  std::string vector_id_mapping = "/anns/head_index/mapping";
  // mapping of the vector id in the head index to the ones in the actual graph.
  // also stores on 1 byte for each vector, its cluster assignment.

  int my_id = -1; // id of this node, logging purpose.
  

  std::string head_index_prefix = "/anns/head_index";
  
  // for testing, this can be just /anns/results, each client id will be: /anns/results/{client_id}, so after a batch of queries completes, we can put the result into /anns/results/{client_id}
  std::string emit_key_prefix_test = "/anns/results";  
  std::string emit_key_prefix = "/anns/graph";  // will need to look into this, reread the affinity set paper.
  
  uint64_t min_batch_size = 1;
  uint64_t max_batch_size = 10;
  uint64_t batch_time_us = 1000;

  // this functions gets the data store data to create the datastore and also the neigbors of each elements to add iteratively to the graph store.
  bool retrieve_and_cache_head_index(DefaultCascadeContextType *typed_ctxt) {
    auto get_data_store_results = typed_ctxt->get_service_client_ref().get(
									   data_store_key, CURRENT_VERSION, true);
    auto &reply = get_data_store_results.get().begin()->second.get();
    Blob data_blob = std::move(const_cast<Blob &>(reply.blob));
    data_blob.memory_mode = object_memory_mode_t::EMPLACED; // what tf is emplaced?
    uint32_t num_pts = *reinterpret_cast<const uint32_t *>(data_blob.bytes);
    uint32_t num_dim =
      *reinterpret_cast<const uint32_t *>(data_blob.bytes + sizeof(uint32_t));
    this->num_pts = num_pts;
    this->dim = num_dim;
    std::cout << "Loaded in " << num_pts << " points with dim "  << num_dim << std::endl;
    auto data_store_ptr = const_cast<data_type *>(reinterpret_cast<const data_type *>(data_blob.bytes + sizeof(uint32_t) * 2));

    std::unique_ptr<diskann::Distance<data_type>> dist;
    dist.reset((diskann::Distance<data_type> *)diskann::get_distance_function<data_type>(diskann::Metric::L2));
    std::shared_ptr<diskann::InMemDataStore<data_type>> data_store =
      std::make_shared(num_pts, num_dim, std::move(dist));
    data_store->populate_data(data_store_ptr, num_pts);
    // done with data store
    
    auto get_graph_store_results = typed_ctxt->get_service_client_ref().get(
									    graph_store_key, CURRENT_VERSION, true);
    auto &graph_reply = get_graph_store_results.get().begin()->second.get();
    Blob graph_blob = std::move(const_cast<Blob &>(graph_reply.blob));
    graph_blob.memory_mode = object_memory_mode_t::EMPLACED;
    uint32_t graph_num_pts =
      *reinterpret_cast<const uint32_t *>(graph_blob.bytes);
    uint32_t max_deg = *reinterpret_cast<const uint32_t *>(graph_blob.bytes +
                                                           sizeof(uint32_t));
    if (graph_num_pts != num_pts) {
      throw std::runtime_error("Difference between data store and graph store pts");
    }
    auto nbr_data_ptr =
        const_cast<uint32_t *>(reinterpret_cast<const uint32_t *>(
								  graph_blob.bytes + sizeof(uint32_t) * 2));
    std::unique_ptr<diskann::InMemGraphStore> graph_store =
      std::make_unique<diskann::InMemGraphStore>(graph_num_pts, max_deg);
    for (uint32_t i = 0; i < graph_num_pts; i++) {
      uint32_t num_nbrs =
          nbr_data_ptr[i * (max_deg + 1)]; // max_deg for all the possielb
      // neighbors, + 1 for the size
      uint32_t *nbr_start_ptr = nbr_data_ptr + i * (max_deg + 1) + 1; 
      uint32_t *nbr_end_ptr = nbr_data_ptr + i * (max_deg + 1) + 1 + num_nbrs;
      std::vector<uint32_t> nbrs(nbr_start_ptr, nbr_end_ptr);
      graph_store->set_neighbours(i, nbrs);
    }
    cached_head_index = true;
  }
  void ocdpo_handler(const node_id_t sender,
                     const std::string &object_pool_pathname,
                     const std::string &key_string,
                     const ObjectWithStringKey &object, const emit_func_t &emit,
                     DefaultCascadeContextType *typed_ctxt,
                     uint32_t worker_id) override {

    if (cached_head_index == false) {
      if (!retrieve_and_cache_head_index(typed_ctxt))
        return;
    }
    // TODO
  }
  static std::shared_ptr<OffCriticalDataPathObserver> ocdpo_ptr;

public:
  std::unique_ptr<BatchingThread> batch_thread;

  static void initialize() {
    if (!ocdpo_ptr) {
      ocdpo_ptr = std::make_shared<HeadIndexSearchOCDPO<data_type>>();
    }
  }
  static auto get() { return ocdpo_ptr; }
  void set_config(DefaultCascadeContextType *typed_ctxt,
                  const nlohmann::json &config);
  void shutdown() {
    for (auto &search_thread : search_threads) {
      if (search_thread) {
        search_thread->signal_stop();
        search_thread->join();
      }
    }
    if (batch_thread) {
      batch_thread->signal_stop();
      batch_thread->join();
    }
  }    
};
template <typename data_type>
std::shared_ptr<OffCriticalDataPathObserver>
    HeadIndexSearchOCDPO<data_type>::ocdpo_ptr;

void initialize(ICascadeContext *ctxt);

std::shared_ptr<OffCriticalDataPathObserver>
get_observer(ICascadeContext *ctxt, const nlohmann::json &config);  

} // namespace cascade
} // namespace derecho
