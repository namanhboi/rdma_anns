#pragma once

#include <cstdint>
#include <cstring>
#include <queue>
#include <memory>
#include <unordered_map>
#include <cascade/service_client_api.hpp>
/*
 * EmbeddingQuery encapsulates a single embedding query that is part of a batch.
 * Operations are performed on demand and directly from the buffer of the whole
 * batch.
 * The metadata is 1 uint64t element (query_id???????? why does this need 64 bits are we bing?) and 3 uint32t elements: client node id, embedding position, embedding size
 *
 */
template<typename data_type>
class EmbeddingQuery {
  std::shared_ptr<uint8_t[]> buffer; // this ptr is the start of a batch of queries, not just this one
  uint64_t buffer_size;
  uint64_t query_id;
  uint32_t client_node_id, embeddings_position, embeddings_size;
  uint64_t dim;
public:
  EmbeddingQuery(std::shared_ptr<uint8_t[]> buffer, uint64_t buffer_size,
                 uint64_t query_id, uint32_t metadata_position) {
    this->buffer = buffer;
    this->buffer_size = buffer_size;
    const uint32_t *metadata = reinterpret_cast<const uint32_t *>(
							    buffer.get() + metadata_position + sizeof(uint64_t));
    this->client_node_id = metadata[0];
    this->embeddings_position = metadata[1];
    this->embeddings_size = metadata[2];
    this->query_id = query_id;

    this->dim = this->embeddings_size / sizeof(data_type);
  }
  uint64_t get_query_id() {
    return this->query_id;
  }
  uint32_t get_client_node_id() { return this->client_node_id; }
  const data_type *get_embedding_ptr() {
    if (this->embeddings_position >= this->buffer_size) return nullptr;
    return reinterpret_cast<const data_type*>(this->buffer.get() + this->embeddings_position);
  }

  uint64_t get_dim() {
    return this->dim;
  }
  
};

/*
 EmbeddingQueryBatchManager perform operations on the whole embedding query
batch received from the client or UDL1.
 Such operations include getting all EmbeddingQuery that are in the batch, or
 getting all the embeddings for processing all in batch.


 Head index search udl recieves a blob that it then deserialize into this object
 to distribute the different queries in this batch to the worker threads.

data layout for a batch of queries looks like the following:
header                          || metadata (for all the queries lined up contiguously) || embeddings [contiguou]
num_queries, embeddings_position   query_id  client_node_id emb_position, emb_size, ..
uint32_t     uint32_t              uint64_t  uint32_t       uint32_t      uint32_t
 *
 */
template<typename data_type>
class EmbeddingQueryBatchManager {
    std::shared_ptr<uint8_t[]> buffer; 
    uint64_t buffer_size;

    uint64_t emb_dim;
    uint32_t num_queries;
    uint32_t embeddings_position;
    bool copy_embeddings = true;

    uint32_t header_size;
    uint32_t metadata_size;
    uint32_t embeddings_size;

    std::vector<std::shared_ptr<EmbeddingQuery<data_type>>> queries;

    void create_queries() {
      for (uint32_t i = 0; i < num_queries; i++) {
        uint32_t metadata_position = header_size + (i * metadata_size);
        uint64_t query_id =
          *reinterpret_cast<uint64_t *>(this->buffer.get() + metadata_position);
        queries.emplace_back(this->buffer, this->buffer_size, query_id, metadata_position);
      }

    }

public:
  EmbeddingQueryBatchManager(const uint8_t *buffer, uint64_t buffer_size,
                             uint64_t emb_dim, bool copy_embeddings = true) { // why would you not want to copty the embedding?
    this->emb_dim = emb_dim;
    this->copy_embeddings = copy_embeddings;

    const uint32_t *header = reinterpret_cast<const uint32_t *>(buffer);
    this->num_queries = header[0];
    this->embeddings_position = header[1];
    this->embeddings_size = buffer_size - this->embeddings_position;
    
    this->header_size = sizeof(uint32_t) * 2;
    this->metadata_size = sizeof(uint32_t) * 3 + sizeof(uint64_t);
    
    if(copy_embeddings){ // why would you not want to copy the embeddings?
        this->buffer_size = buffer_size;
    } else {
        this->buffer_size = buffer_size - this->embeddings_size;
    }

    std::shared_ptr<uint8_t[]> copy(new uint8_t[this->buffer_size]);
    std::memcpy(copy.get(), buffer, this->buffer_size);
    this->buffer = std::move(copy);
  }
  const std::vector<std::shared_ptr<EmbeddingQuery<data_type>>> &get_queries() {
    if (this->queries.empty()) {
      this->create_queries();
    }
    return queries;
  }
  uint32_t get_num_queries() {
    return this->num_queries;
  }

  /** get the embedding position in buffer starting from query number query_id */
  uint32_t get_embeddings_position(uint64_t query_id = 0) {
    return this->embeddings_position +
           (query_id * (this->emb_dim * sizeof(data_type)));
  }
  // get the size of the embeddings buffer for the given number of queries
  uint32_t get_embeddings_size(uint32_t num = 0) {
    return sizeof(data_type) * this->emb_dim * num;
  }
  
};

template <typename data_type>
struct GreedySearchQuery {
  std::byte cluster_id;
  uint32_t starting_node_id;
  std::shared_ptr<EmbeddingQuery<data_type>> query;


  GreedySearchQuery(std::byte _cluster_id, uint32_t _node_id,
                    std::shared_ptr<EmbeddingQuery<data_type>> _query)
  : cluster_id(_cluster_id), starting_node_id(_node_id) {
    query =
        std::move(_query); // we can do this i think because we already got the
                           // head index search result and all that's left is to
                           // move it to the 2nd udl for greedy search.
  }
};

/**
   gathers and serializes the result from Head index search with the query
   embedding to be sent to UDL 2 for greedy searching on the whole graph
 */
template<typename data_type>
class GreedySearchQueryBatcher {
  uint64_t emb_dim;
  uint32_t metadata_size;
  uint32_t header_size;
  uint32_t query_emb_size;
  uint32_t cluster_id_size;
  uint32_t starting_node_id_size;  
  uint32_t num_queries = 0;
  uint32_t total_text_size = 0;
  bool from_buffered = false;

  std::vector<GreedySearchQuery<data_type>> buffered_greedy_search_queries;
  std::shared_ptr<derecho::cascade::Blob> blob;

public:
  GreedySearchQueryBatcher(uint64_t emb_dim, uint64_t size_hint = 1000) {
    this->emb_dim = emb_dim;
    this->metadata_size = sizeof(uint32_t) * 3 + sizeof(uint64_t);
    this->header_size = sizeof(uint32_t) * 2; // num_queries, embeddings_start_position
    this->query_emb_size = sizeof(data_type) * emb_dim;

    this->cluster_id_size = sizeof(std::byte);
    this->starting_node_id_size = sizeof(uint32_t);

    buffered_greedy_search_queries.reserve(size_hint);
  }

  void add_query(std::byte cluster_id, uint32_t starting_node_id,
                 std::shared_ptr<EmbeddingQuery<data_type>> query) {
    buffered_greedy_search_queries.emplace_back(cluster_id, starting_node_id, query);
  }

  std::shared_ptr<derecho::cascade::Blob> get_blob() { return this->blob; }
  
  void serialize() {
    this->num_queries = this->buffered_greedy_search_queries.size();
    uint32_t total_size = this->header_size;
    for (auto &greedy_search_query : buffered_greedy_search_queries) {
      total_size += this->metadata_size + this->cluster_id_size + this->starting_node_id_size+ this->query_emb_size;
    }
    this->blob = std::make_shared<derecho::cascade::Blob>(
        [&](uint8_t *buffer, const std::size_t size) {
	  // data layout: header (num queries, embedding start pos) || metadata each query || cluster id || starting points||emb each query
          uint32_t metadata_position = this->header_size;
          uint32_t cluster_id_position =
            metadata_position + (num_queries * this->metadata_size);
          uint32_t starting_points_position =
            cluster_id_position +  (num_queries * this->cluster_id_size);
          uint32_t embeddings_position =
            starting_points_position + (num_queries * this->starting_node_id_size);

          // write header
	  uint32_t header[2] = {this->num_queries, embeddings_position};
          std::memcpy(buffer, header, this->header_size);

          // write metadata + embedding data for each query
          for (const GreedySearchQuery<data_type> &greedy_search_query : this->buffered_greedy_search_queries) {
            uint64_t query_id = greedy_search_query.query->get_query_id();
            uint32_t client_node_id =
              greedy_search_query.query->get_client_node_id();
            const data_type *query_emb =
              greedy_search_query.query->get_embedding_ptr();

            uint32_t metadata_arr[3] = {client_node_id, embeddings_position,
                                        this->query_emb_size};
	    // copy metadata: query_id, client_node_id, emb_pos, emb_size
            std::memcpy(buffer + metadata_position, &query_id, sizeof(uint64_t));
            std::memcpy(buffer + metadata_position + sizeof(uint64_t),
                        metadata_arr, this->metadata_size - sizeof(uint64_t));
            // copy cluster id
            std::memcpy(buffer + cluster_id_position,
                        &greedy_search_query.cluster_id, this->cluster_id_size);
            // copy starting points
            std::memcpy(buffer + starting_points_position,
                        &greedy_search_query.starting_node_id, this->starting_node_id_size);
            //copy embedding
            std::memcpy(buffer + embeddings_position, query_emb,
                        this->query_emb_size);
            metadata_position += this->metadata_size;
            cluster_id_position += this->cluster_id_size;
            starting_points_position += this->starting_node_id_size;
	    embeddings_position += this->query_emb_size;
          }
          return size; 
	}, total_size);
  }
  void reset() {
    blob.reset();
    buffered_greedy_search_queries.clear();
  }    
};



template <typename data_type>
class GreedySearchQueryBatchManager {

}

